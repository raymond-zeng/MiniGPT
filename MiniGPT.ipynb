{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MiniGPT in PyTorch\n",
        "Today, we'll be creating a decoder only transformer model to generate Shakespearean style text. The transformer model was first proposed in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) which was released by Google and originally intended for machine translation. The original transformer model included an encoder and decoder as exemplified by T5 and BART. The decoder only transformer model was popularlized by OpenAI's GPT series such as GPT-2."
      ],
      "metadata": {
        "id": "D3XyFaTR1tD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/raymond-zeng/MiniGPT/blob/main/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x805RXZS29bt",
        "outputId": "6472bb4d-4b9b-40e6-e8bc-8a7202d69fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-01 22:39:12--  https://github.com/raymond-zeng/MiniGPT/blob/main/input.txt\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt               [  <=>               ]   1.41M  5.59MB/s    in 0.3s    \n",
            "\n",
            "2025-04-01 22:39:14 (5.59 MB/s) - ‘input.txt’ saved [1482575]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "BXfFGHc928QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "lr = 5e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "embed_size = 384\n",
        "num_heads = 6\n",
        "num_layers = 6"
      ],
      "metadata": {
        "id": "7Kq2tVCQ263g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#very very simple tokenization\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s : [char_to_idx[ch] for ch in s]\n",
        "decode = lambda x : ''.join([idx_to_char[i] for i in x])"
      ],
      "metadata": {
        "id": "foHlUegq3hvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split itnto train and validation and batches\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
        "n = int(0.9 * data.size(0))\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "\n",
        "def getBatch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "AnLrZRUO3mhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers biggest innovation was the use of Multi-Head Attention. Multi-Head Attention essentially calculates which of the previous tokens are most important. This is done by keeping key, value pairs and queries. Attention is calculated by the formula: $$\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "where $QK^T$ is the simularity between two tokens. To ensure that the attention mechanism doesn't cheat and look at future tokens, we use a lower triangular matrix as a mask, so that we only calculate attention on previously seen tokens."
      ],
      "metadata": {
        "id": "5bEzDKMN37Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "        weights = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "        return weights @ v\n"
      ],
      "metadata": {
        "id": "hmiNW0nU36t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(embed_size, head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, embed_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out =  self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
        "        return self.dropout(out)"
      ],
      "metadata": {
        "id": "7tbFKgR07jp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see the concept of residual connections which help deal with optimization issues such as gradient vanishing and any difficulties from training deep neural networks. They work by bypassing intermediate layers and adding the original input back to the output of these layers, forming a shortcut connection. This improves gradient flow and helps deep networks learn identity mappings more easily."
      ],
      "metadata": {
        "id": "jxC-8vuzQ54y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = embed_size // num_heads\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads, head_size)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_size, embed_size),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.norm1 = nn.RMSNorm(embed_size)\n",
        "        self.norm2 = nn.RMSNorm(embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.norm1(x))\n",
        "        x = x + self.net(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "XnPmRtJ67qxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_emb = nn.Embedding(block_size, embed_size)\n",
        "        self.blocks = nn.Sequential(*[Block(embed_size, num_heads) for _ in range(num_layers)])\n",
        "        self.norm = nn.RMSNorm(embed_size)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_emb(idx)\n",
        "        pos_emb = self.pos_emb(torch.arange(T, device=device))\n",
        "        x = pos_emb + tok_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B * T, C)\n",
        "          targets = targets.view(B * T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_tokens):\n",
        "        for _ in range(max_tokens):\n",
        "          idx_cond = idx[:, -block_size:]\n",
        "          logits, loss = self(idx_cond)\n",
        "          logits = logits[:, -1, :]\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "F_D4A2hR76iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = getBatch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "model = GPT(vocab_size, embed_size, num_heads, num_layers).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "for i in tqdm.tqdm(range(max_iters)):\n",
        "    X, Y = getBatch('train')\n",
        "    logits, loss = model(X, Y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % eval_interval == 0 or i == max_iters - 1:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"train loss: {losses['train']:.4f} val loss: {losses['val']:.4f}\")\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "text = decode(model.generate(context, max_tokens=500)[0].tolist())\n",
        "print(text)\n",
        "with open('output.txt', 'w') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "DNkdPTzO75u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1213b3-36f1-46b7-89e7-9b0dd72dec4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.807776 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3000 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}